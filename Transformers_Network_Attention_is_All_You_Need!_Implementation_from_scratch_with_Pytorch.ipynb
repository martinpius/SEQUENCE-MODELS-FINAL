{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers Network- Attention is All You Need!..Implementation from scratch with Pytorch",
      "provenance": [],
      "authorship_tag": "ABX9TyM1+xd+r3aKCesfYMGbEilI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/SEQUENCE-MODELS-FINAL/blob/main/Transformers_Network_Attention_is_All_You_Need!_Implementation_from_scratch_with_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZcHxEZFVJdb",
        "outputId": "3a82d14a-96bf-4554-b26d-fc0f455a7594"
      },
      "source": [
        "# Prepare the CoLaB environment, Loading the Google Drive and the GPU device when available:\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import torch\n",
        "  print(f\">>>> You are using Google CoLaB with torch version: {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\">>>> {type(e)} {e}\\n>>>> please correct {type(e)} and reload your device\")\n",
        "  COLAB = False\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "def time_fmt(t: float = 231.123)->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"hrs: {h}, min: {m:>02}, sec: {s:>05.2f}\"\n",
        "print(f\">>>> testing the time formating function...........\\n>>>> time elapsed\\t{time_fmt()}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            ">>>> You are using Google CoLaB with torch version: 1.9.0+cu102\n",
            ">>>> testing the time formating function...........\n",
            ">>>> time elapsed\thrs: 0, min: 03, sec: 51.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt6xIu_MXKw5"
      },
      "source": [
        "# In this notebook we are going to implement a machine translation network (The transformer) from\n",
        "# scratch in Pytorch. The inspiration comes from the paper \"Attention is only you need ==> url == https://arxiv.org/abs/1706.03762\"\n",
        "# Finaly the application with multi30k dataset will be demonstrated.\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7gkm1aqfTmF"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "import numpy as np\n",
        "from tensorflow import summary\n",
        "from tqdm import tqdm\n",
        "import spacy, math\n",
        "import random, time, datetime\n",
        "import os, sys\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FapsGZo8gTtv"
      },
      "source": [
        "# Set the seed values for reproducability and the gpu to deterministic\n",
        "seed = 1234\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0ux1LCCg1AD"
      },
      "source": [
        "# We start by the Multi-head attention: This is the most important part\n",
        "# of the network as all the magic happens. Transformers network archieved\n",
        "# their best performance maily because of the multi-head attention."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh4PuHEm9zS5"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embed_size, heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.heads = heads\n",
        "    self.head_dim = embed_size // heads\n",
        "    assert (self.head_dim * heads == embed_size), \"Embedding size needs to be divisible by heads\"\n",
        "    self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "  \n",
        "  def forward(self, values, keys, query, mask):\n",
        "    # Get number of training examples\n",
        "    N = query.shape[0]\n",
        "    value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "    # Split the embedding into self.heads different pieces\n",
        "    values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "    keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "    query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "    values = self.values(values)  # (N, value_len, heads, head_dim)\n",
        "    keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
        "    queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
        "    # Einsum does matrix mult. for query*keys for each training example\n",
        "    # with every other training example, don't be confused by einsum\n",
        "    # it's just how I like doing matrix multiplication & bmm\n",
        "    energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "    # queries shape: (N, query_len, heads, heads_dim),\n",
        "    # keys shape: (N, key_len, heads, heads_dim)\n",
        "    # energy: (N, heads, query_len, key_len)\n",
        "    # Mask padded indices so their weights become 0\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "      # Normalize energy values similarly to seq2seq + attention\n",
        "      # so that they sum to 1. Also divide by scaling factor for\n",
        "      # better stability\n",
        "      attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "      # attention shape: (N, heads, query_len, key_len)\n",
        "      out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim)\n",
        "      # attention shape: (N, heads, query_len, key_len)\n",
        "      # values shape: (N, value_len, heads, heads_dim)\n",
        "      # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
        "      # we reshape and flatten the last two dimensions.\n",
        "      out = self.fc_out(out)\n",
        "      # Linear layer doesn't modify the shape, final shape will be\n",
        "      # (N, query_len, embed_size)\n",
        "    return out"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8c-wdrI0ahH"
      },
      "source": [
        "# The transformer block: This block is bassically combination of feed-forward nets\n",
        "# we have layers of fc, residual connections and normalization layer:\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embeded, heads, dropout, f_expansion):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.attention = MultiHeadAttention(embeded, heads)\n",
        "    self.norm1 = nn.LayerNorm(embeded)\n",
        "    self.norm2 = nn.LayerNorm(embeded)\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(embeded, f_expansion * embeded),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(f_expansion * embeded, embeded))\n",
        "  \n",
        "  def forward(self, values, keys, query, mask):\n",
        "    attention = self.attention(values, keys, query, mask) # the multihead attention\n",
        "    # add the skip-connection to the layer norm of the attention out\n",
        "    x = self.dropout(self.norm1(attention + query))\n",
        "    # pass the output to the fc layer\n",
        "    f_out = self.fc(x)\n",
        "    # we again add a residual connection and pass in the layer norm \n",
        "    output = self.dropout(self.norm2(f_out + x))\n",
        "    return output\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TDqeLXnWb_Y"
      },
      "source": [
        "# We can now build our network (the encoder, decoder) using the transformer block\n",
        "# which is going to be repeated several times"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KHlbuWLYnDW"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_vocab_size,\n",
        "               embeded_dim,num_layers,\n",
        "               heads, device,f_expansion,\n",
        "               dropout, max_len):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embeded_dim = embeded_dim\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.device = device\n",
        "    # encoder's embedding (the input texts)\n",
        "    self.source_embd = nn.Embedding(input_vocab_size, embeded_dim)\n",
        "    # possitional embedding to make the network invariant to structural changes\n",
        "    self.pos_embd = nn.Embedding(max_len, embeded_dim)\n",
        "    # constructing the transformer (in the decoder's block)\n",
        "    self.layers = nn.ModuleList([\n",
        "                            TransformerBlock(embeded_dim, heads, dropout, f_expansion,)\n",
        "    for _ in range(num_layers)])\n",
        "  \n",
        "  def forward(self, input_tensor, mask):\n",
        "    batch_size, seq_len = input_tensor.shape\n",
        "    pos = torch.arange(0, seq_len).expand(batch_size, seq_len).to(device = device)\n",
        "    out = self.dropout(self.source_embd(input_tensor) + self.pos_embd(pos))\n",
        "    for layer in self.layers:\n",
        "      # for the encoder keys, querry and values are of the same dim\n",
        "      out = layer(out, out, out, mask)\n",
        "    return out\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij-YkZmXmbqx"
      },
      "source": [
        "# The decoder class: Its uses masked multi-head attention from the dec-input and\n",
        "# the output of the encoder's network to produce predictions at every time stamp\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, embeded_size, heads, f_expansion, dropout, device):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "    self.attention = MultiHeadAttention(embeded_size, heads)\n",
        "    self.transformerblock = TransformerBlock(embeded_size, heads, dropout, f_expansion)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.device = device\n",
        "    self.norm1 = nn.LayerNorm(embeded_size)\n",
        "  \n",
        "  def forward(self, x, values, keys, src_mask, trg_mask):\n",
        "    attention = self.attention(x, x, x, trg_mask)\n",
        "    query = self.dropout(self.norm1(attention + x))\n",
        "    out = self.transformerblock(values, keys, query, src_mask)\n",
        "    return out\n",
        "\n",
        "# We can now create our decoder network with the aid of the above block\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, trg_voc_size,embeded_size, num_layers,\n",
        "               heads, f_expansion,dropout, device, max_len):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.device = device\n",
        "    self.dec_embed = nn.Embedding(trg_voc_size, embeded_size)\n",
        "    self.pos_embd = nn.Embedding(max_len, embeded_size)\n",
        "    self.layers = nn.ModuleList([\n",
        "                                DecoderBlock(embeded_size, heads, f_expansion, dropout, device)\n",
        "                                for _ in range(num_layers)])\n",
        "\n",
        "    self.fc_out = nn.Linear(embeded_size, trg_voc_size)\n",
        "\n",
        "  def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "    batch_size, seq_len = x.shape\n",
        "    #define possitional embedding protal-type\n",
        "    pos = torch.arange(0, seq_len).expand(batch_size, seq_len).to(device = device)\n",
        "    x = self.dropout(self.dec_embed(x) + self.pos_embd(pos))\n",
        "    # decoder layers\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
        "    out = self.fc_out(x)\n",
        "    return out\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJYFO3XNuofI"
      },
      "source": [
        "# We now combine the above classes to build the final model-class:\n",
        "class TrfModel(nn.Module):\n",
        "  def __init__(self, src_voc_size,\n",
        "               trg_voc_size, src_pad_idx,\n",
        "               trg_pad_idx,\n",
        "               embed_size = 256,\n",
        "               num_layers = 6,\n",
        "               f_expansion = 4,\n",
        "               heads = 8, dropout = 0,\n",
        "               device = device,\n",
        "               max_len = 100):\n",
        "    super(TrfModel,self).__init__()\n",
        "    self.encoder = Encoder(src_voc_size,\n",
        "                           embed_size, \n",
        "                           num_layers,\n",
        "                           heads,\n",
        "                           device, \n",
        "                           f_expansion, \n",
        "                           dropout, max_len)\n",
        "    self.decoder = Decoder(trg_voc_size, embed_size,\n",
        "                           num_layers, heads,\n",
        "                           f_expansion, dropout,\n",
        "                           device, max_len)\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "    self.device = device\n",
        "  \n",
        "  def src_mask_build(self, src_input):\n",
        "    src_mask = (src_input != self.src_pad_idx).unsqueeze(1).unsqueeze(2) # shape == [batch_size, 1, 1, src_len]\n",
        "    return src_mask.to(device = self.device)\n",
        "  \n",
        "  def trg_mask_build(self, trg_input):\n",
        "    # its uses lower triangular matrix-type\n",
        "    batch_size, trg_len = trg_input.shape\n",
        "    trg_mask = torch.tril(torch.ones(trg_len, trg_len)).expand(batch_size, 1, trg_len, trg_len)\n",
        "    return trg_mask.to(device = self.device)\n",
        "  \n",
        "  def forward(self, src, trg):\n",
        "    src_mask = self.src_mask_build(src) # build the mask for the input sequence\n",
        "    trg_mask = self.trg_mask_build(trg) # build the mask for the output sequence\n",
        "    enc_src = self.encoder(src, src_mask)\n",
        "    outputs = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "    return outputs\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX5gIjO6KiED",
        "outputId": "c28133aa-8d79-49ab-aac4-91d254723611"
      },
      "source": [
        "# Instantiating and testing the model class:\n",
        "source_input = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 0], [1, 8, 7, 3, 4, 5, 6, 7]]).to(device = device)\n",
        "target = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 0], [1, 8, 7, 3, 4, 5, 7, 2]]).to(device = device)\n",
        "src_pad_idx = 0\n",
        "trg_pad_idx = 0\n",
        "src_voc_size = 10\n",
        "trg_voc_size = 10\n",
        "model = TrfModel(src_voc_size, trg_voc_size, src_pad_idx, trg_pad_idx).to(device = device)\n",
        "#outputs = model(source_input, target[:, : -1])\n",
        "outputs = model(source_input, target[:, :-1])\n",
        "print(f\">>>> the desired output shape: {outputs.shape}\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>> the desired output shape: torch.Size([2, 7, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpRTI4nR_Orm"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}